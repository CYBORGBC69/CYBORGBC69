{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1938459,
          "sourceType": "datasetVersion",
          "datasetId": 1111894
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CYBORGBC69/CYBORGBC69/blob/main/Copy_of_Examples_of_Feature_Selection_in_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "fedesoriano_company_bankruptcy_prediction_path = kagglehub.dataset_download('fedesoriano/company-bankruptcy-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "X9zHPpioImS8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am presenting some feature selection methods, that can be used in classification and regression problems when the datasets have too many columns. Feature selection is important because it can:\n",
        "* Enhance model performance\n",
        "* Reduce overfitting\n",
        "* Improve the model interpretability\n",
        "* Make the model training faster\n"
      ],
      "metadata": {
        "id": "_vaY_VK8ImS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-02-25 alle 11.32.08.jpg](attachment:9f06b297-b366-4032-9439-5b3b56c9f9cc.jpg)"
      ],
      "metadata": {
        "id": "mtpHYQadImS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### SUMMARY\n",
        "1. [Feature Selection with Feature Importance](#1)\n",
        "2. [Feature Selection with Mutual Information Score](#2)\n",
        "3. [Feature Selection with Correlation Heatmap](#3)\n",
        "4. [Feature Selection with Variance Threshold](#4)\n",
        "5. [Sequential Feature Selection](#5)\n",
        "6. [Classification with a Random Forest Classifier](#6)\n",
        "---"
      ],
      "metadata": {
        "id": "7U-yRkrbImTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9exRGK5-ImTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Feature Selection with Feature Importance\n",
        "<a id=\"1\"></a>"
      ],
      "metadata": {
        "id": "pvyLupJCImTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Brief EDA"
      ],
      "metadata": {
        "id": "DNr2Su9xImTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I am using a dataset with a very large number of columns. It describes the financial situation of companies and their bankruptcy status: 0 stands for no bankruptcy, 1 for bankruptcy."
      ],
      "metadata": {
        "id": "vqUL8m6qImTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')\n",
        "\n",
        "data1.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "54EVphvgImTC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The dataset has {data1.shape[0]} rows and {data1.shape[1]} columns.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "7j17JEjlImTC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The dataset has {data1.isna().sum().sum()} null values.')\n",
        "\n",
        "print()\n",
        "\n",
        "print(f'The dataset has {data1.duplicated().sum()} duplicate rows.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "k6iq4I3QImTC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [col for col in data1.columns if data1[col].dtypes == 'O']\n",
        "\n",
        "print(f'There are {len(cat_cols)} categorical columns in the dataset.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "gkeFR-mcImTD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cols = []\n",
        "\n",
        "for col in data1.columns:\n",
        "    if data1[col].nunique() == 2:\n",
        "        binary_cols.append(col)\n",
        "\n",
        "print(f'There are {len(binary_cols)} binary columns.')\n",
        "print(f'They are: {binary_cols}.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "1NrpBdp1ImTD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Bankrupt?' is the target variable.\n",
        "\n",
        "More details on the dataset, including a complete exploratory data analysis, can be found in **Ref. 1**."
      ],
      "metadata": {
        "id": "NzdoaOGIImTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before carrying out classification, it is necessary to reduce the number of columns by dropping those that are unnecessary and/or redundant. To do that, I will make use of feature importance calculated with a random forest classifier."
      ],
      "metadata": {
        "id": "D3c4LqS7ImTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Feature Selection with Feature Importance"
      ],
      "metadata": {
        "id": "bPBjx-cRImTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I am defining *X* and *y* ..."
      ],
      "metadata": {
        "id": "pp59iiwgImTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data1.drop('Bankrupt?', axis=1)\n",
        "y = data1['Bankrupt?']"
      ],
      "metadata": {
        "trusted": true,
        "id": "VFJBrgF5ImTE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "... then, I am computing the feature importances by means of a random forest classifier."
      ],
      "metadata": {
        "id": "4lQVCumNImTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# To sort the index in descending order, I multiply 'rf.feature_importances_' by -1\n",
        "sorted_idx = (-rf.feature_importances_).argsort()\n",
        "\n",
        "list_of_tuples = list(zip(X.columns[sorted_idx],\n",
        "                          rf.feature_importances_[sorted_idx]))\n",
        "\n",
        "feat_importance = pd.DataFrame(list_of_tuples,\n",
        "                  columns=['feature', 'feature importance'])\n",
        "\n",
        "##################\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "fig = sns.barplot(data=feat_importance[feat_importance['feature importance'] > 0.015], x='feature', y='feature importance')\n",
        "plt.title('Feature Importance > 0.015',fontsize=25)\n",
        "plt.xticks(fontsize=8,rotation=60)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2UeBKQ5xImTE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I can get a list of the features with importance greater than a given threshold, like 0.01 or 0.015 ..."
      ],
      "metadata": {
        "id": "LE3zRTDjImTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_001 = feat_importance[feat_importance['feature importance'] > 0.01]['feature'].to_list()\n",
        "\n",
        "col_02 = feat_importance[feat_importance['feature importance'] > 0.02]['feature'].to_list()\n",
        "\n",
        "print('Features with importance > 0.01: ')\n",
        "print(col_001)\n",
        "print()\n",
        "print('Features with importance > 0.02: ')\n",
        "print(col_02)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AagHDZ7fImTE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "... or I can directly get the 'small' *X*."
      ],
      "metadata": {
        "id": "VmTjFI1WImTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[col_02].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "9UR-JFVNImTE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe same feature selection method can be applied to regression by substituting the RandomForestClassifier with a RandomForestRegressor."
      ],
      "metadata": {
        "id": "iylc4vBbImTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Feature Selection with Permutation-Based Importance"
      ],
      "metadata": {
        "id": "FmBPn_asImTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same can be done by using permutation-based importance."
      ],
      "metadata": {
        "id": "DwUBkW5XImTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perm_importance = permutation_importance(rf, X, y)\n",
        "\n",
        "sorted_idx = (-perm_importance.importances_mean).argsort()\n",
        "\n",
        "list_of_tuples  = list(zip(X.columns[sorted_idx],\n",
        "                           perm_importance.importances_mean[sorted_idx]))\n",
        "\n",
        "perm_importance = pd.DataFrame(list_of_tuples,\n",
        "                  columns=['feature','permutation importance'])\n",
        "\n",
        "print(perm_importance.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "UCQSbzrNImTF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "sns.barplot(perm_importance[perm_importance['permutation importance'] > 0.0005], x='feature', y='permutation importance')\n",
        "\n",
        "plt.title('Permutation-Based Importances > 0.0005', fontsize=25)\n",
        "plt.xlabel('feature', fontsize=15)\n",
        "plt.xticks(fontsize=8, rotation=45)\n",
        "plt.ylabel('permutation importance', fontsize=15)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "trusted": true,
        "id": "LJnMWXwzImTF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "col_0005 = perm_importance[perm_importance['permutation importance'] > 0.0005]['feature'].to_list()\n",
        "\n",
        "print('Features with permutation importance > 0.0005: ')\n",
        "col_0005"
      ],
      "metadata": {
        "trusted": true,
        "id": "geQvzqSYImTF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Selection with Mutual Information Score\n",
        "<a id=\"2\"></a>"
      ],
      "metadata": {
        "id": "O4hjAVaWImTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An  alternative to feature importance is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target. Then one can choose a smaller set of the most useful features."
      ],
      "metadata": {
        "id": "n3xmJ-5JImTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discrete_features = X.dtypes == int\n",
        "\n",
        "def make_mi_scores(X, y, discrete_features):\n",
        "    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n",
        "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
        "    mi_scores = mi_scores.sort_values(ascending=False)\n",
        "    return mi_scores\n",
        "\n",
        "mi_scores = make_mi_scores(X, y, discrete_features).reset_index()\n",
        "\n",
        "mi_scores.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CBvPSAKSImTF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, I am plotting the mutual information scores > 0.025."
      ],
      "metadata": {
        "id": "j_RvXjhNImTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.figure(figsize=(12,9))\n",
        "\n",
        "ax = sns.barplot(data=mi_scores[mi_scores['MI Scores'] > 0.025], x='index', y='MI Scores')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, size=7)\n",
        "ax.set_title('Mutual Information Scores > 0.025', size=30)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "trusted": true,
        "id": "-39dBD94ImTF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The names of the scores above a given threshold can be used to create a list of the most relevant features."
      ],
      "metadata": {
        "id": "Ymu8kHGmImTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores_0025 = mi_scores[mi_scores['MI Scores'] > 0.025]['index'].to_list()\n",
        "\n",
        "mi_scores_0025"
      ],
      "metadata": {
        "trusted": true,
        "id": "D1KjI_kmImTG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the list of features, one should investigate the existence of possible interactions among the features and also play with the threshold (here set at 0.0025). Investigating the correlations between the features is very important and I will discuss it in the following sections.\n",
        "\n",
        "This method can also be applied to regression problems, by substituting the 'mutual_info_classif' function with 'mutual_info_regression'."
      ],
      "metadata": {
        "id": "n_4F7R_gImTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Selection with Correlation Heatmap\n",
        "<a id=\"3\"></a>"
      ],
      "metadata": {
        "id": "JNGeIo1UImTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am printing the correlation heatmaps of *X* for each series of relevant (or selected) features that I have obtained with the previous methods."
      ],
      "metadata": {
        "id": "5Od5JNBLImTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(X[col_02].corr(method='pearson'),annot=True,fmt='.2f',annot_kws={\"fontsize\":8},cmap='Blues')\n",
        "plt.title('Correlation heatmap',fontsize=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dTlyCN2SImTJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(X[col_0005].corr(method='pearson'),annot=True,fmt='.2f',annot_kws={\"fontsize\":8},cmap='Reds')\n",
        "plt.title('Correlation heatmap',fontsize=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eG4rKmNhImTJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(X[mi_scores_0025].corr(method='pearson'),annot=True,fmt='.2f',annot_kws={\"fontsize\":6},cmap='Greens')\n",
        "plt.title('Correlation heatmap',fontsize=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "v2Ssd45aImTJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will drop the columns with *r* > 0.9. However, given that each value of *r* results from the correlation of two different columns, I will only keep one of the columns from each pair, that with a higher value of feature importance, mutual information score ...\n",
        "\n",
        "In the case of the first heatmap, the blue one, where the number of highly correlated features is low, I can drop the extra features 'by hand'. Only two of them exhibit a very high correlation (*r* > 0.9): they are ' Net profit before tax/Paid-in capital' and ' Persistent EPS in the Last Four Seasons'. I am keeping only one of them, the one that has the highest value of feature importance."
      ],
      "metadata": {
        "id": "-4PJ8TnVImTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importance[feat_importance['feature importance'] > 0.02]"
      ],
      "metadata": {
        "trusted": true,
        "id": "s_OyMOkwImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "col_02.remove(' Net profit before tax/Paid-in capital')\n",
        "\n",
        "col_02"
      ],
      "metadata": {
        "trusted": true,
        "id": "8xAY-TEHImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the resulting dataframe."
      ],
      "metadata": {
        "id": "qs-F0xXeImTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[col_02]"
      ],
      "metadata": {
        "trusted": true,
        "id": "SgiuhktLImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the third case, that of the green correlation heatmap, the number of highly correlated features is larger and thus this procedure should be automated. Let's try to do this."
      ],
      "metadata": {
        "id": "3ZYT32aGImTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows, cols = X[mi_scores_0025].shape\n",
        "flds = list(X[mi_scores_0025].columns)\n",
        "\n",
        "corr = X[mi_scores_0025].corr().values\n",
        "\n",
        "cols_to_drop_list = []\n",
        "\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "        if corr[i,j] > 0.9:\n",
        "            mi_scores_i = float(mi_scores[mi_scores['index'] == flds[i]]['MI Scores'])\n",
        "            mi_scores_j = float(mi_scores[mi_scores['index'] == flds[j]]['MI Scores'])\n",
        "            print(flds[i], ' ', flds[j], ' ', corr[i,j])\n",
        "            if mi_scores_i > mi_scores_j:\n",
        "                cols_to_drop_list.append(flds[j])\n",
        "            else:\n",
        "                cols_to_drop_list.append(flds[i])\n",
        "\n",
        "cols_to_drop_list"
      ],
      "metadata": {
        "trusted": true,
        "id": "3HNXn_fCImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mi_scores_0025_cut = [x for x in mi_scores_0025 if x not in cols_to_drop_list]\n",
        "\n",
        "len(mi_scores_0025), len(mi_scores_0025_cut)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OBsyVU1AImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have cut almost half of the columns in the 'mi_scores_0025' list."
      ],
      "metadata": {
        "id": "jdMMie-GImTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Selection with Variance Threshold\n",
        "<a id=\"4\"></a>"
      ],
      "metadata": {
        "id": "9eXbjrGHImTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variance threshold method removes all those features whose variance does not meet some threshold. By default, it removes all zero-variance features, i.e., features with the same value in all samples under the assumption that the features with a higher variance may contain more useful information.\n",
        "\n",
        "For quasi-constant features, that have the same value for a very large subset, using a threshold of 0.01 would mean dropping the column where 99% of the values are similar."
      ],
      "metadata": {
        "id": "Ub23F2nAImTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_thr = VarianceThreshold(threshold = 0.1)\n",
        "var_thr.fit(X)\n",
        "\n",
        "var_thr.get_support()"
      ],
      "metadata": {
        "trusted": true,
        "id": "gxQp1McRImTK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values in the output mean:\n",
        "* True: High Variance\n",
        "* False: Low Variance"
      ],
      "metadata": {
        "id": "AWd7sZhqImTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am dropping the columns that are 90% or more similar."
      ],
      "metadata": {
        "id": "xAZn9xUoImTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concol = [column for column in X.columns if column not in X.columns[var_thr.get_support()]]\n",
        "\n",
        "for features in concol:\n",
        "    print(features)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kpgKsK0IImTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cols_after_var_thr = [x for x in X.columns if x not in concol]\n",
        "\n",
        "len(cols_after_var_thr), len(X.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DJUhgGtqImTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final number of columns after applying variance threshold is around 1/4 of the initial number."
      ],
      "metadata": {
        "id": "SQE42XeZImTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(X[cols_after_var_thr].corr(method='pearson'),annot=True,fmt='.2f',annot_kws={\"fontsize\":6},cmap='Blues')\n",
        "plt.title('Correlation heatmap',fontsize=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "69AoADQ_ImTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Sequential Feature Selection\n",
        "<a id=\"5\"></a>"
      ],
      "metadata": {
        "id": "GWQDBM22ImTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Feature Selector is a greedy procedure where, at each iteration, one chooses the best new feature to add to the selected features based a cross-validation score. One starts with 0 features and chooses the best single feature with the highest score. The procedure is repeated until one reaches the desired number of selected features."
      ],
      "metadata": {
        "id": "bHWJrchIImTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs_forward = SequentialFeatureSelector(rf, n_features_to_select=7, direction=\"forward\").fit(X, y)\n",
        "\n",
        "selected_features_forw = [column for column in X.columns if column in X.columns[sfs_forward.get_support()]]\n",
        "\n",
        "for features in selected_features_forw:\n",
        "    print(features)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pwsM-SlHImTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X[selected_features_forw]"
      ],
      "metadata": {
        "trusted": true,
        "id": "k22NPz18ImTL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have commented the code below because it is quite slow. To make it run faster, one should substitute the random forest classifier with another one."
      ],
      "metadata": {
        "id": "XxjHK82UImTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''sfs_backward = SequentialFeatureSelector(rf, n_features_to_select=7, direction=\"backward\").fit(X, y)\n",
        "\n",
        "selected_features_back = [column for column in X.columns if column in X.columns[sfs_backward.get_support()]]\n",
        "\n",
        "for features in selected_features_back:\n",
        "    print(features)'''"
      ],
      "metadata": {
        "trusted": true,
        "id": "IXSEG_tUImTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Classification with a Random Forest Classifier\n",
        "<a id=\"6\"></a>"
      ],
      "metadata": {
        "id": "sqfIXnzMImTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Classification with the 'Feature Importance' Dataset"
      ],
      "metadata": {
        "id": "rzLvZAmrImTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_scores(model_name:str,preds,y_test_data):\n",
        "    '''\n",
        "    Generate a table of test scores.\n",
        "\n",
        "    In:\n",
        "        model_name (string): Your choice: how the model will be named in the output table\n",
        "        preds: numpy array of test predictions\n",
        "        y_test_data: numpy array of y_test data\n",
        "\n",
        "    Out:\n",
        "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
        "    '''\n",
        "    accuracy  = accuracy_score(y_test_data,preds)\n",
        "    precision = precision_score(y_test_data,preds,average='macro')\n",
        "    recall    = recall_score(y_test_data,preds,average='macro')\n",
        "    f1        = f1_score(y_test_data,preds,average='macro')\n",
        "\n",
        "    table = pd.DataFrame({'model': [model_name],'precision': [precision],'recall': [recall],\n",
        "                          'F1': [f1],'accuracy': [accuracy]})\n",
        "\n",
        "    return table"
      ],
      "metadata": {
        "trusted": true,
        "id": "omXaH0jKImTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[col_02], y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mjb9dfqRImTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "rf_test_preds_FI = rf.predict(X_test)\n",
        "\n",
        "rf_test_results_FI = get_test_scores('Random Forest (with Feature Importance)', rf_test_preds_FI, y_test)\n",
        "\n",
        "rf_test_results_FI"
      ],
      "metadata": {
        "trusted": true,
        "id": "yZLyZhoqImTM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall score is quite low. To improve it, one should under-/over-sample the train data. See **Ref. 2**."
      ],
      "metadata": {
        "id": "3kbCG8bfImTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Classification with the 'Forward Feature Selection' Dataset"
      ],
      "metadata": {
        "id": "NSiZXEtwImTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[selected_features_forw], y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "oBaFxKYdImTN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(X_train, y_train)\n",
        "\n",
        "rf_test_preds_FFS = rf.predict(X_test)\n",
        "\n",
        "rf_test_results_FFS = get_test_scores('Random Forest (with Forward Feature Selection)', rf_test_preds_FFS, y_test)\n",
        "\n",
        "rf_test_results_FFS"
      ],
      "metadata": {
        "trusted": true,
        "id": "lBxHnhdrImTN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Jacopo Ferretti, [*Causes of Stroke: Logistic Regression + Partial Dependence + SHAP*](https://www.kaggle.com/code/jacopoferretti/causes-of-stroke-log-regr-partial-dependence-shap), notebook on Kaggle.\n",
        "2. Jacopo Ferretti, [*Company Bankruptcy: Classification with Feature Selection*](https://www.kaggle.com/code/jacopoferretti/company-bankruptcy-classif-w-feature-selection), notebook on Kaggle.\n",
        "3. Ryan Holbrook and Alexis Cook, [*Feature Engineering*](https://www.kaggle.com/learn/feature-engineering), course on Kaggle.\n",
        "4. Shelvi Garg, [*Dropping Constant Features using VarianceThreshold: Feature Selection -1*](https://medium.com/nerd-for-tech/removing-constant-variables-feature-selection-463e2d6a30d9), article on medium.com.\n",
        "5. Manoj Kumar, Maria Telenczuk and Nicolas Hug, [*Model-based and sequential feature selection*](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py), scikit-learn.org."
      ],
      "metadata": {
        "id": "HbK7kzVKImTN"
      }
    }
  ]
}